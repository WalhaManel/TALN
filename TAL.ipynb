{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b270df1e",
   "metadata": {},
   "source": [
    "# Importation des bibliothéques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3ad9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize,WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import spacy\n",
    "from nltk.stem.snowball import FrenchStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6213d7",
   "metadata": {},
   "source": [
    "# Imporation et Fussionnement des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214cb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=r\"C:\\Users\\21652\\Desktop\\TAl\\data\"\n",
    "r=[f for f in os.listdir(path) if os.path.isfile(os.path.join(path,f))]\n",
    "def create_data(r):\n",
    "    data=[]\n",
    "    for i in r:\n",
    "        p=path+'\\\\'+i\n",
    "        d=open(p,'r')\n",
    "        data.append(d.read().lower())\n",
    "    return data\n",
    "data=create_data(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda202b",
   "metadata": {},
   "source": [
    "# Tokenization & droping stop-words & Lemmatisation & Stremming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a721c3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fonction qui permet de séparer une séquence (texte) en une liste de tokens (mots)\n",
    "'''\n",
    "def tokenization(data):\n",
    "    tk = WhitespaceTokenizer()\n",
    "    data_words=[tk.tokenize(row) for row in data ]\n",
    "    return data_words\n",
    "'''\n",
    "Fonction de nettoyage des stop words et des punctuations\n",
    "'''\n",
    "def clean(doc):\n",
    "    #crow=[r for r in doc if r.isalpha()]\n",
    "    crow=[re.sub(r\"[]\"+ string.punctuation + r\"]\", \" \",r) for r in doc]\n",
    "    crow=[re.sub(r\"[0-9©@~&£$*%§°#’™·•–®]\",\" \",w) for w in crow]\n",
    "    crow=[re.sub(r'\\s+', ' ',  w) for w in crow]\n",
    "    mysp=stopwords.words('french')+stopwords.words('english')\n",
    "    crow=[w for w in crow if (w not in mysp) & (w !=' ') & (w not in [\"tout\",\"plus\",\"cet\",\"chez\",\"d un\",\"ci\",\"celle\",\"…\",\"d une\",\"celui\",\"dun\",\"dans\"])]\n",
    "    return crow\n",
    "'''\n",
    "Fonction de lemmatization des mots\n",
    "'''\n",
    "def lemmatization(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cdoc=[lemmatizer.lemmatize(w, pos='v') for w in doc]\n",
    "    return cdoc\n",
    "'''\n",
    "Fonctions de Racinisation\n",
    "'''\n",
    "def stremmin(doc):\n",
    "    sb = FrenchStemmer()\n",
    "    return [sb.stem(w) for w in doc]\n",
    "#appel des fonctions\n",
    "data1=tokenization(data)\n",
    "data1=[lemmatization(doc) for doc in data1]\n",
    "data1=[stremmin(doc) for doc in data1]\n",
    "data1=[clean(doc) for doc in data1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3a882",
   "metadata": {},
   "source": [
    "# Extraction de la fréquence des termes simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c36c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frequences=[FreqDist(doc) for doc in data1 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4855a9d",
   "metadata": {},
   "source": [
    "# Extraction des termes composés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14b3c5",
   "metadata": {},
   "source": [
    "# Méthode Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb599df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "# Exractions des keyphrases et calcul de score en fonction de la fréquence des mots et des cooccurrences\n",
    "rake = Rake(stopwords=stopwords.words('french')+stopwords.words('english'), max_length=4)\n",
    "\n",
    "keyphrases=[]\n",
    "for doc in data1:\n",
    "    rake.extract_keywords_from_sentences(doc)\n",
    "    rake_keyphrases =rake.get_ranked_phrases_with_scores()\n",
    "    keyphrases.append({t[1]:t[0] for t in rake_keyphrases if t[1].find(' ')!=-1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade7747",
   "metadata": {},
   "source": [
    "# Méthode spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71f4f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41341b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "#cleaned data\n",
    "data2=[str(\" \".join(str(w) for w in doc)) for doc in data1]\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "for text in data2:\n",
    "    doc=nlp(text)\n",
    "# examine the top-ranked phrases in the document\n",
    "keyphrases=set()\n",
    "for phrase in doc._.phrases:\n",
    "    keyphrases.add(phrase.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc16be2",
   "metadata": {},
   "source": [
    "# Méthode yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a4cb949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('malad trait vni', 1.1765335478296876e-07),\n",
       "  ('trait vni malad', 2.8759708946947927e-07),\n",
       "  ('vni malad trait', 2.8759708946947927e-07),\n",
       "  ('dan group ventil', 3.2052104061245403e-07),\n",
       "  ('courb pression volum', 3.322721890588002e-07),\n",
       "  ('pression dan voi', 3.4088635665650596e-07),\n",
       "  ('dan group malad', 3.6995577142453463e-07),\n",
       "  ('insuffis respiratoir aigu', 4.586543577226987e-07),\n",
       "  ('cour ventil mécan', 4.6369052736982713e-07),\n",
       "  ('patient atteint bpco', 4.987627101432221e-07),\n",
       "  ('utilis vni malad', 5.283376192382639e-07),\n",
       "  ('dan étud contrôl', 5.497059655496522e-07),\n",
       "  ('group patient ventil', 5.611662951393337e-07),\n",
       "  ('patient ventil ech', 5.690954490178647e-07),\n",
       "  ('dan group patient', 5.940902700786524e-07),\n",
       "  ('dan trait malad', 6.04188768354008e-07),\n",
       "  ('dan group vni', 6.460825013069689e-07),\n",
       "  ('utilis vni dan', 6.461251631857553e-07),\n",
       "  ('pression expiratoir posit', 6.482847941884545e-07),\n",
       "  ('élev dan group', 6.54376165571431e-07)],\n",
       " [('pression artériel pulmonair', 1.4460695230802285e-08),\n",
       "  ('éject ventriculair gauch', 5.924609555644872e-08),\n",
       "  ('respiratoir pression artériel', 6.747359114875869e-08),\n",
       "  ('pression artériel systol', 9.914633004811376e-08),\n",
       "  ('télédiastol ventriculair gauch', 1.130688586604646e-07),\n",
       "  ('télédiastol ventriculair droit', 1.1660697302045826e-07),\n",
       "  ('remplissag ventriculair gauch', 1.2759919673295841e-07),\n",
       "  ('pression télédiastol ventriculair', 1.3053943163302582e-07),\n",
       "  ('pression auriculair droit', 1.3522623208590245e-07),\n",
       "  ('pression auriculair gauch', 1.444258578448389e-07),\n",
       "  ('éject ventriculair droit', 1.6060439209918924e-07),\n",
       "  ('flux veineux pulmonair', 1.6130552940594815e-07),\n",
       "  ('cœur pulmonair aigu', 1.6332446238677504e-07),\n",
       "  ('volum télédiastol ventriculair', 1.781929040451113e-07),\n",
       "  ('pression ventriculair gauch', 1.8961091480493818e-07),\n",
       "  ('pression artériel puls', 1.9317785086620397e-07),\n",
       "  ('variabl respiratoir pression', 1.9830343177532952e-07),\n",
       "  ('droit pression artériel', 2.04351022241356e-07),\n",
       "  ('œdem aigus pulmonair', 2.1781498670421385e-07),\n",
       "  ('précharg ventriculair gauch', 2.4014591998211777e-07)],\n",
       " [('dan liquid céphalorachidien', 1.1850638034099044e-06),\n",
       "  ('cocc gram posit', 1.1962960951871068e-06),\n",
       "  ('trait infect staphylocoqu', 1.6025878478309452e-06),\n",
       "  ('souch sensibil diminu', 1.7715216978541943e-06),\n",
       "  ('résist lactamin résist', 1.7804381530134976e-06),\n",
       "  ('pneumocoqu sensibil diminu', 1.925720750412812e-06),\n",
       "  ('view within articl', 1.9811832500686526e-06),\n",
       "  ('impliqu dan infect', 2.244437017810605e-06),\n",
       "  ('résist cocc gram', 2.3042755551176517e-06),\n",
       "  ('résist efflux résist', 2.4275602863347525e-06),\n",
       "  ('résist fluoroquinolon résist', 2.5054624705036674e-06),\n",
       "  ('posit dan cas', 2.5448619056868562e-06),\n",
       "  ('infect staphylocoqu résist', 2.5499671282632234e-06),\n",
       "  ('impliqu dan résist', 2.801302729983715e-06),\n",
       "  ('trait antibiot trait', 2.8331371307677314e-06),\n",
       "  ('dan cas dan', 2.9083995115925898e-06),\n",
       "  ('dan trait infect', 3.000182559773388e-06),\n",
       "  ('résist naturel résist', 3.1376799663249553e-06),\n",
       "  ('full siz tabl', 3.1786798853324866e-06),\n",
       "  ('résist glycopeptid résist', 3.421108510263667e-06)],\n",
       " [('dan group trait', 2.1175506797965807e-07),\n",
       "  ('utilis dan cas', 2.781342215514008e-07),\n",
       "  ('dan trait syndrom', 2.915624168163238e-07),\n",
       "  ('édit scientif médical', 4.6222350732621995e-07),\n",
       "  ('dan group placebo', 4.6935843416541563e-07),\n",
       "  ('présent dan cas', 4.732362577335833e-07),\n",
       "  ('scientif médical elsevi', 4.768729143046152e-07),\n",
       "  ('médical elsevi sas', 5.262212718348937e-07),\n",
       "  ('dan cas syndrom', 5.383301445204678e-07),\n",
       "  ('cas trait dan', 5.891544706021885e-07),\n",
       "  ('mortal dan group', 5.961414200800051e-07),\n",
       "  ('dan pris charg', 6.106101009282655e-07),\n",
       "  ('accident ischem cérébral', 6.594813370142439e-07),\n",
       "  ('utilis dan trait', 6.77879781147411e-07),\n",
       "  ('ischem cérébral dan', 7.150448106773685e-07),\n",
       "  ('dan cas patient', 7.288537426595468e-07),\n",
       "  ('heur apres début', 7.483919864161531e-07),\n",
       "  ('réanim édit scientif', 7.640335757945025e-07),\n",
       "  ('cérébral patient trait', 8.088742224181623e-07),\n",
       "  ('deb sanguin cérébral', 8.375923733649401e-07)]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yake import KeywordExtractor as Yake\n",
    "#cleaned data\n",
    "data2=[str(\" \".join(str(w) for w in doc)) for doc in data1]\n",
    "yake = Yake(lan='fr')\n",
    "yake_keyphrases=[]\n",
    "for doc in data2:\n",
    "    yake_keyphrases.append(yake.extract_keywords(doc))\n",
    "\n",
    "yake_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40146f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
